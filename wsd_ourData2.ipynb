{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/firststef/APLN-WSD/blob/master/wsd_ourData2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy sklearn rowordnet\n",
        "!python -m spacy download ro_core_news_lg\n",
        "!pip install bs4\n",
        "!pip install sparknlp pyspark"
      ],
      "metadata": {
        "id": "F1hfFcVwCFi7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nlEYrj0n8zzy"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"ro_core_news_lg\")\n",
        "\n",
        "\n",
        "def parse_xml(path):\n",
        "  with open(path, encoding='utf8') as fp:\n",
        "    file_p = fp.read()\n",
        "    soup = BeautifulSoup(file_p, 'html.parser')\n",
        "  data = soup.find_all('instance')\n",
        "  parsed_data = {}\n",
        "  for sense in data:\n",
        "    new_data = []\n",
        "    contexts = sense.find_all('context')\n",
        "    senseId = sense.attrs[\"id\"][10:]\n",
        "    if len(senseId)<=1:\n",
        "      continue\n",
        "    word = ''\n",
        "    for context in contexts:\n",
        "      try:\n",
        "        word = nlp(context.head.get_text())[0].lemma_\n",
        "        sentence = context.decode_contents().strip('\\n')\n",
        "\n",
        "        #print(word)\n",
        "        a, *b = sentence.split('<head>')\n",
        "        b, *c = ''.join(b).split('</head>')\n",
        "        sentence = a + b + ''.join(c)\n",
        "        sentence= ' '.join(sentence.split())\n",
        "        new_data.append((sentence, senseId))\n",
        "        \n",
        "      except Exception as e:\n",
        "        print(e)\n",
        "        continue\n",
        "    if word =='':\n",
        "      continue\n",
        "    if word in parsed_data:\n",
        "      parsed_data[word].extend(new_data)\n",
        "    else:\n",
        "      parsed_data[word] = new_data\n",
        "     \n",
        "  return parsed_data\n",
        "\n",
        "\n",
        "\n",
        "prep_data = parse_xml(\"/content/drive/MyDrive/Colab Notebooks/wsd/toate.xml\")\n",
        "print(len(prep_data.keys()))\n",
        "print(prep_data[\"lac\"][0])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "def split_data(data):\n",
        "  train_data = {}\n",
        "  test_data = {}\n",
        "  for word, vector in data.items():\n",
        "    senses = {}\n",
        "    for tup in vector:\n",
        "      context, sense = tup\n",
        "      if sense not in senses:\n",
        "        senses[sense] = []\n",
        "      senses[sense].append(tup)\n",
        "\n",
        "    train_data[word] = []\n",
        "    test_data[word] = []\n",
        "\n",
        "    # print(senses)\n",
        "\n",
        "    for sense in senses:\n",
        "      vec = senses[sense]\n",
        "      l = len(vec)\n",
        "      marg = 2*l//3\n",
        "      if l == 1:\n",
        "        train_data[word].extend(vec[0])\n",
        "        test_data[word].extend(vec[0])\n",
        "      else:\n",
        "        random.shuffle(vec)\n",
        "        ex_train = vec[:marg]\n",
        "        ex_test = vec[marg:]\n",
        "        train_data[word].extend(ex_train)\n",
        "        test_data[word].extend(ex_test)\n",
        "  return train_data, test_data\n",
        "\n",
        "train_data, test_data = split_data(prep_data)\n",
        "print(train_data[\"lac\"])\n",
        "print(test_data[\"lac\"])"
      ],
      "metadata": {
        "id": "UzGEtRZyO6jv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import spacy\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "import rowordnet as rwn\n",
        "from nltk.classify import MaxentClassifier\n",
        "\n",
        "vec = DictVectorizer()\n",
        "wn = rwn.RoWordNet()\n",
        "nlp = spacy.load(\"ro_core_news_lg\")\n",
        "\n",
        "\n",
        "def extract_hypernymes(word):\n",
        "    hyper = []\n",
        "    synset_ids = wn.synsets(literal=word.lemma_, pos=rwn.synset.Synset.Pos.NOUN)\n",
        "    if len(synset_ids) > 0:\n",
        "        c = wn.synset_to_hypernym_root(synset_ids[0])\n",
        "        for c1 in c:\n",
        "            if len(wn(c1).literals):\n",
        "                hyper.append(wn(c1).literals[0])\n",
        "    hyper_dict = {}\n",
        "    for i in range(len(hyper)):\n",
        "      hyper_dict[\"hyper\"+str(i)] = hyper[i]\n",
        "    #print(hyper_dict)\n",
        "    return hyper_dict\n",
        "\n",
        "\n",
        "def extract_feat_for_word(word, rel):\n",
        "    hyper = {}\n",
        "    if word.pos_ == \"NOUN\":\n",
        "        hyper = extract_hypernymes(word)\n",
        "    d = {\n",
        "        (\"text\"+rel): word.text,\n",
        "        \"pos\"+rel: word.pos_,\n",
        "        \"morpho\"+rel: word.morph,\n",
        "        \"lemma\"+rel: word.lemma_,\n",
        "        \"dep\"+rel: word.dep_,\n",
        "         \"ner\"+rel: word.ent_type_,\n",
        "        #\"hyper\"+rel: hyper,\n",
        "        #\"position_rel\": rel\n",
        "    }\n",
        "    d.update(hyper)\n",
        "    return d\n",
        "\n",
        "def create_features_vector(word, context):\n",
        "    text = nlp(context)\n",
        "    vect = {}\n",
        "    for w in text:\n",
        "      if w.lemma_ == word:\n",
        "            if w.head != w:\n",
        "              \n",
        "              #print(w.head)\n",
        "              vect.update(extract_feat_for_word(w.head, \"head\"))\n",
        "              for ind, siblings in enumerate(w.head.children):\n",
        "                vect.update(extract_feat_for_word(siblings, \"0_\"+str(ind) if w != siblings else \"main\"))\n",
        "            else:\n",
        "              vect.update(extract_feat_for_word(w, \"main\"))\n",
        "            for chil in w.children:\n",
        "              vect.update(extract_feat_for_word(chil, \"children\"))\n",
        "        \n",
        "    return vect\n",
        "\n",
        "def prepare_data(data):\n",
        "  new_data_per_words = {}\n",
        "\n",
        "  for word, instances in data.items():\n",
        "    sense = word\n",
        "    #print(sense)\n",
        "    new_data = []\n",
        "    #print(instances)\n",
        "    \n",
        "    for i in instances:\n",
        "      #print(i)\n",
        "      #print(i[0])\n",
        "      new_data.append((create_features_vector(sense,i[0]), i[1]))\n",
        "    new_data_per_words[word] = new_data\n",
        "\n",
        "\n",
        "  return new_data_per_words\n",
        "\n",
        "a = prepare_data(train_data)\n",
        "\n",
        "#test_data, test_labels = prepare_test_data(test_se3)\n",
        "b = prepare_data(test_data)\n",
        "print(a[\"lac\"])\n",
        "print(b[\"lac\"])\n",
        "print(a.keys())"
      ],
      "metadata": {
        "id": "kbEIQ-KXXKGj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "algorithm = nltk.classify.MaxentClassifier.ALGORITHMS[0]\n",
        "classifiers = {}\n",
        "for w in a.keys():\n",
        "  print(w)\n",
        "  classifier = nltk.MaxentClassifier.train(a[w], algorithm, max_iter=20)\n",
        "  #classifier.show_most_informative_features(40, show='all')\n",
        "  #print(classifier.weights())\n",
        "  #print(feat)\n",
        "  classifiers[w] = classifier\n",
        "\n",
        "all_accs = []\n",
        "for w in b.keys():\n",
        "  output_labels = classifiers[w].classify_many([x[0] for x in b[w]])\n",
        "  acc = nltk.classify.accuracy( classifiers[w], b[w])\n",
        "  all_accs.append(acc)\n",
        "\n",
        "print(\"Precizie\", avg(all_accs))"
      ],
      "metadata": {
        "id": "EkNN-3uCYEPM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import rowordnet as rwn\n",
        "wn = rwn.RoWordNet()\n",
        "a = wn('ENG30-00982602-a').literals\n",
        "print(a)"
      ],
      "metadata": {
        "id": "R7whmlc8AKdk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"ro_core_news_lg\")\n",
        "title = nlp('mÄƒrii')\n",
        "print(title[0].lemma_)\n"
      ],
      "metadata": {
        "id": "iqU0SnnBBjL7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}