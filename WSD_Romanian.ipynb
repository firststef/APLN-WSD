{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPCroYb47INdfmnh52aW6G7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/firststef/APLN-WSD/blob/master/WSD_Romanian.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yoMogo0mHQU0"
      },
      "outputs": [],
      "source": [
        "#!pip install spark-nlp==4.2.3 pyspark==3.2.1\n",
        "#!pip3 install pybuilder\n",
        "#!git clone https://github.com/paudan/opennlp_python.git\n",
        "#!cd opennlp_python && pyb \n",
        "!wget http://setup.johnsnowlabs.com/colab.sh -O - | bash"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U setuptools wheel\n",
        "!pip install -U spacy\n",
        "!python -m spacy download ro_core_news_md"
      ],
      "metadata": {
        "id": "B9Miise4pKe8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sparknlp\n",
        "import nltk\n",
        "import spacy\n",
        "spark = sparknlp.start()\n",
        "nltk.download('punkt')\n",
        "spnlp = spacy.load('ro_core_news_md')"
      ],
      "metadata": {
        "id": "U7kR3zKPlwAv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51f34f12-27f8-4a04-f4d5-633dec75e7a5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sparknlp.base import *\n",
        "from sparknlp.annotator import *\n",
        "from sparknlp.pretrained import PretrainedPipeline\n",
        "\n",
        "def get_word_embeddings(sentence):\n",
        "  documentAssembler = DocumentAssembler().setInputCol(\"text\").setOutputCol(\"document\")\n",
        "\n",
        "  tokenizer = Tokenizer() \\\n",
        "  .setInputCols(\"document\") \\\n",
        "  .setOutputCol(\"token\")\n",
        "\n",
        "  embeddings = WordEmbeddingsModel.pretrained(\"w2v_cc_300d\",\"ro\") \\\n",
        "  .setInputCols([\"document\", \"token\"]) \\\n",
        "  .setOutputCol(\"embeddings\")\n",
        "\n",
        "  pipeline = Pipeline(stages=[documentAssembler, tokenizer, embeddings])\n",
        "\n",
        "  data = spark.createDataFrame([[sentence]]).toDF(\"text\")\n",
        "\n",
        "  result = pipeline.fit(data).transform(data)\n",
        "\n",
        "  return result"
      ],
      "metadata": {
        "id": "-_6DpvUTgC5G"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = get_word_embeddings(\"Propozitie de exemplu\")\n",
        "result.select('embeddings').show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "id": "rDVESa9_kwYt",
        "outputId": "0cf5e5dd-a444-4c99-ad91-7e08781ee26f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-791ca1c3b589>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_word_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Propozitie de exemplu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'embeddings'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'get_word_embeddings' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "def truncate_sentence_semantic(sentence, word):\n",
        "  toks = word_tokenize(sentence)\n",
        "  index = toks.index(word)\n",
        "  left = index - 3\n",
        "  left = 0 if left < 0 else left\n",
        "  right = index + 4\n",
        "  right = len(toks) if right >= len(toks) else right\n",
        "  return \" \".join(toks[left:right])\n",
        "\n",
        "truncate_sentence_semantic(\"Ca hobby, obisnuiesc sa merg la sala unde trag cat pot de mine!\", \"sala\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "bDBEyJ4El_mX",
        "outputId": "dba13ffe-f78d-4d84-b7f0-cfa836f665d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'sa merg la sala unde trag cat'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sintactic_semantic(sentence):\n",
        "  doc = spnlp(sentence)\n",
        "  return ([x.pos_ for x in doc], [(x.text,x.label_) for x in doc.ents])\n",
        "\n",
        "print(get_sintactic_semantic(\"El este Ștefan, vine din Romania.\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8nYkYcvrp3ye",
        "outputId": "237db651-c095-4d8e-bc2e-9828b4848766"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(['PRON', 'AUX', 'PROPN', 'PUNCT', 'AUX', 'ADP', 'PROPN', 'PUNCT'], [('El', 'PERSON'), ('Ștefan', 'GPE'), ('Romania', 'GPE')])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "#nltk.download('mte_teip5')\n",
        "print(nltk.corpus.multext_east.tagged_words(\"oana-ro.xml\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XLSkC6bT34-D",
        "outputId": "1e55da6c-b5ff-4020-f71d-5a567e9a69d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Într-', '#Spsay'), ('o', '#Tifsr'), ...]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk, nltk.classify.util, nltk.metrics\n",
        "from nltk.classify import MaxentClassifier\n",
        "from nltk.collocations import BigramCollocationFinder\n",
        "from nltk.metrics import BigramAssocMeasures\n",
        "from nltk.probability import FreqDist, ConditionalFreqDist\n",
        "from nltk.classify import MaxentClassifier\n",
        "\n",
        "from nltk.corpus import senseval\n",
        "\n",
        "# from nltk.corpus import movie_reviews\n",
        "\n",
        "# def word_feats(words):\n",
        "#  return dict([(word, True) for word in words])\n",
        "\n",
        "# negids = movie_reviews.fileids('neg')\n",
        "# posids = movie_reviews.fileids('pos')\n",
        "\n",
        "# negfeats = [(word_feats(movie_reviews.words(fileids=[f])), 'neg') for f in negids]\n",
        "# posfeats = [(word_feats(movie_reviews.words(fileids=[f])), 'pos') for f in posids]\n",
        "\n",
        "# negcutoff = len(negfeats)*3//4\n",
        "# poscutoff = len(posfeats)*3//4\n",
        "\n",
        "# trainfeats = negfeats[:negcutoff] + posfeats[:poscutoff]\n",
        "\n",
        "algorithm = nltk.classify.MaxentClassifier.ALGORITHMS[0]\n",
        "classifier = nltk.MaxentClassifier.train(trainfeats, algorithm,max_iter=3)\n",
        "\n",
        "classifier.show_most_informative_features(10)\n",
        "\n",
        "# all_words = nltk.FreqDist(word for word in movie_reviews.words())\n",
        "# top_words = set(all_words.keys()[:300])\n",
        "\n",
        "# def word_feats(words):\n",
        "#     return {word:True for word in words if word in top_words}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OqR9zC7sxtDe",
        "outputId": "0aeb3f00-177e-400c-fc80-225ff0ffad0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ==> Training (3 iterations)\n",
            "\n",
            "      Iteration    Log Likelihood    Accuracy\n",
            "      ---------------------------------------\n",
            "             1          -0.69315        0.500\n",
            "             2          -0.69252        0.953\n",
            "         Final          -0.69190        0.953\n",
            "  -0.000 magnificent==True and label is 'neg'\n",
            "  -0.000 insulting==True and label is 'pos'\n",
            "  -0.000 vulnerable==True and label is 'neg'\n",
            "  -0.000 uninvolving==True and label is 'pos'\n",
            "  -0.000 avoids==True and label is 'neg'\n",
            "  -0.000 outstanding==True and label is 'neg'\n",
            "  -0.000 astounding==True and label is 'neg'\n",
            "  -0.000 fascination==True and label is 'neg'\n",
            "  -0.000 ludicrous==True and label is 'pos'\n",
            "  -0.000 affecting==True and label is 'neg'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# am de parsat sens-eval 3 si pus in algoritmul de antrenare cumva"
      ],
      "metadata": {
        "id": "tShAFQ1BDwjg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}